{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bT5mABNB5R7e"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogi8celw4_U-",
        "outputId": "bb4230ad-4256-4a56-9a8c-1bfe05fdee75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-02-01 04:34:07.454539: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-01 04:34:07.454598: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-01 04:34:07.457134: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-01 04:34:07.468256: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-02-01 04:34:10.026489: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-lg==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.6.0/en_core_web_lg-3.6.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.10.14)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.1.4)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUy3KeGN519T",
        "outputId": "3bec2c9c-703b-4dd1-a359-a8abcd2ee15b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pandas==1.5.1\n",
        "!pip install -q pyLDAvis==3.4.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzQsfYuCJkcm",
        "outputId": "fae713c3-3098-4f93-b75e-4596d27ce02e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==1.5.1\n",
            "  Downloading pandas-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.1) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.1) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.1) (1.16.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 1.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.5.1\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzVWRd4V5Bd3",
        "outputId": "73518a5d-0fc6-4888-dae8-8f1637674b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                review_id                 user_id             business_id  \\\n",
            "0  IVS7do_HBzroiCiymNdxDg  fdFgZQQYQJeEAshH4lxSfQ  sGy67CpJctjeCWClWqonjA   \n",
            "1  QP2pSzSqpJTMWOCuUuyXkQ  JBLWSXBTKFvJYYiM-FnCOQ  3w7NRntdQ9h0KwDsksIt5Q   \n",
            "2  oK0cGYStgDOusZKz9B1qug  2_9fKnXChUjC5xArfF8BLg  OMnPtRGmbY8qH_wIILfYKA   \n",
            "3  E_ABvFCNVLbfOgRg3Pv1KQ  9MExTQ76GSKhxSWnTS901g  V9XlikTxq0My4gE8LULsjw   \n",
            "4  Rd222CrrnXkXukR2iWj69g  LPxuausjvDN88uPr-Q4cQA  CA5BOxKRDPGJgdUQ8OUOpw   \n",
            "\n",
            "   stars  useful  funny  cool  \\\n",
            "0      3       1      1     0   \n",
            "1      5       1      1     1   \n",
            "2      5       1      0     0   \n",
            "3      5       0      0     0   \n",
            "4      4       1      0     0   \n",
            "\n",
            "                                                text                 date  \n",
            "0  OK, the hype about having Hatch chili in your ...  2020-01-27 22:59:06  \n",
            "1  Pandemic pit stop to have an ice cream.... onl...  2020-04-19 05:33:16  \n",
            "2  I was lucky enough to go to the soft opening a...  2020-02-29 19:43:44  \n",
            "3  I've gone to claim Jumpers all over the US and...  2020-03-14 21:47:07  \n",
            "4  If you haven't been  to Maynard's kitchen, it'...  2020-01-17 20:32:57  \n",
            "(48147, 9)\n",
            "              stars        useful         funny          cool\n",
            "count  48147.000000  48147.000000  48147.000000  48147.000000\n",
            "mean       3.736702      0.858683      0.183106      0.439903\n",
            "std        1.557289      1.831488      0.807035      1.451746\n",
            "min        1.000000      0.000000      0.000000      0.000000\n",
            "25%        2.000000      0.000000      0.000000      0.000000\n",
            "50%        5.000000      0.000000      0.000000      0.000000\n",
            "75%        5.000000      1.000000      0.000000      0.000000\n",
            "max        5.000000    105.000000     55.000000    106.000000\n"
          ]
        }
      ],
      "source": [
        "# Code cell 1\n",
        "import spacy\n",
        "import nltk\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "import seaborn as sb\n",
        "import pyLDAvis.lda_model\n",
        "from gensim import corpora, models\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "rra = r'/content/drive/MyDrive/restaurant_reviews_az.csv'\n",
        "df= pd.read_csv(rra)\n",
        "\n",
        "# Summary\n",
        "print(df.head(5))\n",
        "print(df.shape)\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk-uqALk5dws",
        "outputId": "425c4633-f8da-4a03-8007-7cc4d7d064b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# Code cell 2\n",
        "selected_reviews = df[(df['stars'] == 1) | (df['stars'] == 5)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8R0UmEu69FY",
        "outputId": "a8ded38d-cf39-4e65-ff81-873555af4ef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# Code cell 3\n",
        "# Perform text preprocessing - remove stopwords and tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Tokenize and remove stopwords and punctuation\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "selected_reviews['processed_text'] = selected_reviews['text'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNxEKIIY9R0l",
        "outputId": "306e2989-3fa6-4aaf-cacc-19b2006c2a6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "#Code cell 4\n",
        "# Use Count Vectorizer to represent the reviews\n",
        "vectorizer = CountVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(selected_reviews['processed_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQWPHlXV9mBu",
        "outputId": "65930801-1bad-4c5c-b134-8d8af5a64cca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 20 words in 1-star reviews: ['food', 'order', 'time', 'get', 'would', 'place', 'back', 'service', 'us', 'one', 'like', 'ordered', 'go', 'never', 'even', 'got', 'said', 'minutes', 'could', 'restaurant']\n",
            "Top 20 words in 5-star reviews: ['food', 'great', 'place', 'good', 'service', 'delicious', 'amazing', 'back', 'best', 'time', 'tucson', 'love', 'also', 'go', 'always', 'one', 'friendly', 'staff', 'restaurant', 'definitely']\n"
          ]
        }
      ],
      "source": [
        "# Code cell 5\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "word_counts = X.toarray()\n",
        "\n",
        "one_star_counts = word_counts[selected_reviews['stars'] == 1].sum(axis=0)\n",
        "five_star_counts = word_counts[selected_reviews['stars'] == 5].sum(axis=0)\n",
        "\n",
        "one_star_top_words = [feature_names[idx] for idx in one_star_counts.argsort()[::-1][:20]]\n",
        "five_star_top_words = [feature_names[idx] for idx in five_star_counts.argsort()[::-1][:20]]\n",
        "\n",
        "print(\"Top 20 words in 1-star reviews:\", one_star_top_words)\n",
        "print(\"Top 20 words in 5-star reviews:\", five_star_top_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "wY7B4Rcv_0Lg",
        "outputId": "fe1a0717-0392-43c6-db71-5bac2269f5e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(n_jobs=-1, random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_jobs=-1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_jobs=-1, random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Code Cells 6\n",
        "# Set the number of topics to 10 and train an LDA model for 10 iterations\n",
        "num_topics = 10\n",
        "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42, n_jobs=-1, max_iter=10)\n",
        "lda.fit(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEHdYlvJE-7t",
        "outputId": "614d76f2-a217-4b87-b218-9f0feed5b256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: us, food, minutes, back, came, service, asked, table, said, would\n",
            "Topic 2: order, food, time, get, would, service, customer, go, location, called\n",
            "Topic 3: great, dinner, us, restaurant, patio, delicious, night, menu, nice, wine\n",
            "Topic 4: chicken, good, delicious, food, rice, also, ordered, sauce, spicy, fresh\n",
            "Topic 5: pizza, food, like, chicken, place, ordered, wings, good, sauce, got\n",
            "Topic 6: food, great, service, place, amazing, friendly, good, delicious, staff, back\n",
            "Topic 7: burger, fries, sandwich, cheese, salad, ordered, bacon, steak, bread, got\n",
            "Topic 8: tacos, food, good, mexican, place, salsa, burrito, taco, best, delicious\n",
            "Topic 9: always, place, food, tucson, love, best, one, go, get, good\n",
            "Topic 10: coffee, cream, like, ice, tea, breakfast, good, place, got, also\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# Code Cell 7\n",
        "# Show top 10 words generated by each topic\n",
        "for i, topic in enumerate(lda.components_):\n",
        "    top_words_idx = topic.argsort()[::-1][:10]\n",
        "    top_words = [feature_names[idx] for idx in top_words_idx]\n",
        "    print(f\"Topic {i+1}: {', '.join(top_words)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtAqnOm8IUS8",
        "outputId": "cec8fc33-8310-4665-bdf6-cfd3f825f646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review 1: pandemic pit stop ice cream plain sundae limited menu written screens outside unpleasant surprise cashier wearing gloves mask holding item good since hold lid three customers pm location bomb parking access easy great visibility pictures tonight soo\n",
            "Topic Distribution: [0.00277857 0.24790169 0.00277878 0.00277831 0.00277837 0.00277872\n",
            " 0.00277823 0.00277859 0.34587146 0.38677728]\n",
            "==================================================\n",
            "Review 2: lucky enough go soft opening let tell good beer wine many different modern italian appetizers bruschettas paninis salads please favor visit place good staff friendly look forward enjoying restaurant future\n",
            "Topic Distribution: [0.00344896 0.00344897 0.56710854 0.00344908 0.00344918 0.0034494\n",
            " 0.0034489  0.00344911 0.40529867 0.00344919]\n",
            "==================================================\n",
            "Review 3: gone claim jumpers us never disappoint location different cook food well excellent desserts service deliver everything impeccable would absolutely recommend restaurant quality meal best place tucson take wife dinner hands\n",
            "Topic Distribution: [0.00333469 0.0033343  0.00333459 0.00333392 0.00333443 0.58031803\n",
            " 0.00333402 0.00333401 0.39300826 0.00333376]\n",
            "==================================================\n",
            "Review 4: stay main hotel casino july july worst experience ever years supported hotel casino however time disaster go back hopefullly\n",
            "Topic Distribution: [0.00555736 0.4235942  0.3130956  0.00555604 0.00555743 0.00555652\n",
            " 0.00555618 0.00555641 0.0055574  0.22441287]\n",
            "==================================================\n",
            "Review 5: town long weekend hiking camping burgers well seasoned cooked ordered lots flavor waitresses super friendly attentive expecting bar scene pleasantly surprised relaxing dinner locals\n",
            "Topic Distribution: [0.00416772 0.00416702 0.10670342 0.00416807 0.00416741 0.45897857\n",
            " 0.26885962 0.14045314 0.00416742 0.00416762]\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# Code Cell 8\n",
        "# Show the document topic distribution of the first 5 reviews in your dataset\n",
        "document_topic_distribution = lda.transform(X[:5])\n",
        "\n",
        "for i, review in enumerate(selected_reviews['processed_text'][:5]):\n",
        "    print(f\"Review {i+1}: {review}\")\n",
        "    print(\"Topic Distribution:\", document_topic_distribution[i])\n",
        "    print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9c8pQ7TIZ1x",
        "outputId": "92d00b7a-0bbf-4e13-f4fb-a0596079f859"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Topics: 3, Iterations: 5, Perplexity: 536.456299893303\n",
            "Number of Topics: 3, Iterations: 7, Perplexity: 532.1865636338417\n",
            "Number of Topics: 3, Iterations: 9, Perplexity: 530.5813157114258\n",
            "Number of Topics: 4, Iterations: 5, Perplexity: 545.0654018446597\n",
            "Number of Topics: 4, Iterations: 7, Perplexity: 538.5424189633152\n",
            "Number of Topics: 4, Iterations: 9, Perplexity: 535.2640860150893\n",
            "Number of Topics: 5, Iterations: 5, Perplexity: 549.8309606374969\n",
            "Number of Topics: 5, Iterations: 7, Perplexity: 542.6128898252567\n",
            "Number of Topics: 5, Iterations: 9, Perplexity: 538.6311079661227\n",
            "\n",
            "Best Model Configuration: Number of Topics = 3, Iterations = 9\n"
          ]
        }
      ],
      "source": [
        "# Code Cell 9: Experiment with different number of topics and iterations\n",
        "best_model = None\n",
        "best_score = -1  # Initialize with a low value\n",
        "\n",
        "# Adjust the range and step size for quicker experimentation\n",
        "for num_topics in range(3, 6):  # Experiment with 3 to 5 topics\n",
        "    for num_iterations in range(5, 11, 2):  # Experiment with iterations from 5 to 10 in steps of 2\n",
        "        vectorizer = CountVectorizer(max_features=1000)  # Adjust max_features based on available memory\n",
        "        X = vectorizer.fit_transform(selected_reviews['processed_text'])\n",
        "\n",
        "        lda = LatentDirichletAllocation(n_components=num_topics, random_state=42, n_jobs=-1, max_iter=num_iterations)\n",
        "        lda.fit(X)\n",
        "\n",
        "        # Use perplexity as a scoring metric\n",
        "        perplexity_score = lda.perplexity(X)\n",
        "\n",
        "        print(f\"Number of Topics: {num_topics}, Iterations: {num_iterations}, Perplexity: {perplexity_score}\")\n",
        "\n",
        "        if perplexity_score < best_score or best_score == -1:\n",
        "            best_score = perplexity_score\n",
        "            best_model = lda\n",
        "\n",
        "print(f\"\\nBest Model Configuration: Number of Topics = {best_model.n_components}, Iterations = {best_model.max_iter}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "panel = pyLDAvis.lda_model.prepare(best_model, X, vectorizer, mds='tsne')\n",
        "pyLDAvis.display(panel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "id": "hvzQon26pVLH",
        "outputId": "12d252a9-baf5-40e9-be81-3d2979b62f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el1691373201387769608181189967\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el1691373201387769608181189967_data = {\"mdsDat\": {\"x\": [-63.88837432861328, -0.5085275769233704, 554.3494873046875], \"y\": [337.2738952636719, -339.850830078125, 51.35410690307617], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [36.455440669283014, 33.246307033466344, 30.298252297250656]}, \"tinfo\": {\"Term\": [\"great\", \"order\", \"chicken\", \"food\", \"friendly\", \"amazing\", \"delicious\", \"service\", \"minutes\", \"ordered\", \"always\", \"sauce\", \"best\", \"love\", \"staff\", \"cheese\", \"said\", \"asked\", \"tucson\", \"good\", \"told\", \"us\", \"rice\", \"got\", \"salad\", \"place\", \"manager\", \"tacos\", \"flavor\", \"fresh\", \"told\", \"rude\", \"waited\", \"worst\", \"horrible\", \"terrible\", \"poor\", \"manager\", \"refund\", \"employee\", \"disgusting\", \"minutes\", \"min\", \"mins\", \"attitude\", \"awful\", \"charged\", \"dirty\", \"receipt\", \"mcdonald\", \"worse\", \"zero\", \"asked\", \"bill\", \"showed\", \"paid\", \"waiting\", \"ridiculous\", \"card\", \"upset\", \"called\", \"walked\", \"understand\", \"phone\", \"girl\", \"money\", \"said\", \"management\", \"lady\", \"bad\", \"call\", \"problem\", \"order\", \"customers\", \"us\", \"left\", \"orders\", \"someone\", \"mask\", \"thru\", \"never\", \"took\", \"could\", \"would\", \"even\", \"another\", \"table\", \"know\", \"get\", \"customer\", \"time\", \"went\", \"people\", \"one\", \"food\", \"like\", \"came\", \"back\", \"go\", \"got\", \"service\", \"ordered\", \"restaurant\", \"place\", \"wait\", \"pastries\", \"atmosphere\", \"welcoming\", \"knowledgeable\", \"environment\", \"vibe\", \"ambiance\", \"brunch\", \"accommodating\", \"outdoor\", \"fun\", \"attentive\", \"friendly\", \"cocktails\", \"selection\", \"music\", \"comfortable\", \"beautiful\", \"efficient\", \"notch\", \"cafe\", \"exceptional\", \"helpful\", \"owned\", \"pancakes\", \"margaritas\", \"outstanding\", \"great\", \"cute\", \"hidden\", \"awesome\", \"patio\", \"always\", \"breakfast\", \"clean\", \"amazing\", \"love\", \"wonderful\", \"staff\", \"prices\", \"tucson\", \"excellent\", \"best\", \"highly\", \"food\", \"service\", \"favorite\", \"place\", \"recommend\", \"spot\", \"town\", \"nice\", \"delicious\", \"good\", \"coffee\", \"definitely\", \"menu\", \"restaurant\", \"super\", \"back\", \"really\", \"go\", \"time\", \"well\", \"one\", \"try\", \"everything\", \"also\", \"asada\", \"boba\", \"broth\", \"juicy\", \"tofu\", \"crispy\", \"pastor\", \"texture\", \"veggies\", \"creamy\", \"pork\", \"brisket\", \"seasoned\", \"noodles\", \"carne\", \"birria\", \"tender\", \"al\", \"crunchy\", \"tomato\", \"bean\", \"quesadilla\", \"roasted\", \"soft\", \"grilled\", \"pad\", \"peppers\", \"shredded\", \"ramen\", \"veggie\", \"beans\", \"rice\", \"spicy\", \"fried\", \"potato\", \"beef\", \"sauce\", \"cheese\", \"garlic\", \"perfectly\", \"chicken\", \"milk\", \"green\", \"flavor\", \"corn\", \"shrimp\", \"salad\", \"taco\", \"meat\", \"tea\", \"tacos\", \"soup\", \"burrito\", \"cooked\", \"fries\", \"ordered\", \"hot\", \"also\", \"good\", \"got\", \"sandwich\", \"delicious\", \"fresh\", \"side\", \"sweet\", \"try\", \"bread\", \"like\", \"really\", \"definitely\", \"pizza\", \"back\", \"place\", \"well\", \"food\", \"time\", \"one\", \"great\", \"best\"], \"Freq\": [13078.0, 8358.0, 4950.0, 22325.0, 4495.0, 5833.0, 7595.0, 10571.0, 3058.0, 6729.0, 5163.0, 2868.0, 5964.0, 5033.0, 5188.0, 2695.0, 3148.0, 2566.0, 5734.0, 11563.0, 2322.0, 5991.0, 1949.0, 6147.0, 2503.0, 13052.0, 1985.0, 2463.0, 2129.0, 3696.0, 2321.002951771272, 1126.9963246553546, 1276.761918214307, 994.9815658445185, 816.057341850877, 838.9985523620762, 444.0846004385908, 1981.7713016841424, 365.21767978546524, 494.9089700691785, 347.20106153504264, 3051.4431189916963, 413.95873937118114, 332.1571520075835, 416.93556228187543, 404.9373908068863, 313.169336634899, 370.9540600753497, 235.2431498120754, 221.24784954087002, 236.14736558884167, 317.76515935092556, 2555.9321859323227, 383.24870755539644, 289.61613938989217, 600.897317116945, 1307.165251519453, 229.79901481403783, 428.74599384514283, 215.5446624465731, 1378.609384900442, 833.9917795626603, 478.7197153775126, 839.1854310163368, 435.6661171700182, 1050.5679697741914, 3014.8700856477635, 501.12329845881266, 549.0605386536466, 1629.3278181568628, 881.598219118374, 437.32532990741555, 6803.037202600546, 1188.0290790715287, 4760.7094117715, 1203.606764414294, 1008.0254868002229, 799.231303813972, 636.7302709205309, 617.3997363397066, 3332.311783904317, 1754.537624038652, 2990.284809127344, 4168.457504907726, 3273.6059465996477, 1442.83899555739, 1761.2164678140073, 1892.4152897860279, 4243.289614834398, 1772.469558570295, 4490.21978506286, 2292.3173344113793, 1973.7386200556507, 3653.250806410988, 6788.376400644096, 3196.705033488019, 2260.791303431744, 3637.587395830454, 3141.6843557823213, 2821.800633635394, 3428.665009303722, 2560.727413621979, 2295.3773711064996, 2872.2717062158595, 1899.7152044670459, 218.86839551537201, 1465.9622640841883, 402.1024277902402, 213.99590621588231, 325.419172467475, 405.3866194731745, 455.4502969912628, 584.9729201745225, 271.43704014767644, 601.8680397724858, 803.3164325242552, 957.9811485513804, 4167.238029818904, 315.61868917250393, 684.6927194458008, 480.59925855177426, 325.1478751274998, 648.1886835286547, 248.65188572899226, 250.91064525309864, 381.2071397639831, 248.85203843768693, 783.5175043721518, 268.7521854049998, 303.95254945319607, 289.9800917081631, 598.2984332612182, 11243.066266236367, 370.4936672682256, 226.5171745670499, 1521.0689668482635, 1004.3002297699869, 4315.621192391946, 1843.283938911133, 1402.0878647101995, 4686.176527802701, 4027.728052132441, 1388.874313774741, 4076.0697424453383, 1038.463127803396, 4280.765780876615, 2054.900213282874, 4273.927564842595, 1573.8003601138616, 13127.53985856609, 6742.9787782044805, 2124.9030319146314, 7836.097228268403, 2616.7332248497064, 1464.7968591257204, 1417.775295445568, 2439.520582511658, 4040.1696375204806, 5479.465883661316, 1226.3958237311729, 2554.0198349348825, 2250.657462037005, 3014.9393789215355, 1640.77695032027, 3086.713381175905, 2230.682908261412, 2580.981221012485, 2599.9622937871627, 1802.1124204832004, 2022.3826816971068, 1802.98500064452, 1680.1697382445461, 1787.190883787028, 714.3440909361192, 405.7374540752134, 382.4880190964626, 315.20571186804017, 279.00415092968024, 850.8719150555922, 233.6485664201973, 285.78362927588677, 423.1399337144115, 310.5004176621021, 1329.8576618170393, 452.4621298789709, 426.4113712647142, 742.6473545452462, 763.5384087589041, 338.0356093728821, 638.9773937483635, 242.82365840782182, 287.73446980524125, 363.5326455768624, 226.61931546978602, 274.5181391751242, 237.54490986322077, 540.9510827533509, 678.8682015956284, 220.90204633630114, 233.8272693964564, 220.68466109935542, 819.2818573131651, 391.2162918182467, 1059.7328095663577, 1907.8634212302143, 1461.5195323067533, 1350.6410767480916, 583.8549184117344, 1521.8744042390242, 2717.8827666242155, 2515.663661782689, 791.576090076997, 1088.496403548231, 4427.465795921403, 444.25575641440935, 901.7875477685267, 1935.67529285282, 520.9486021763121, 1209.7702760442078, 2233.837122576867, 1066.676350007262, 1727.8504439208302, 915.340167724684, 2098.344276365848, 1019.3169017867289, 1005.1113413454997, 1295.8806339710331, 1578.8187016758648, 4099.627656021826, 1638.7456840114796, 3111.4676271431845, 4932.872831057658, 3162.4376460402045, 1336.435673581957, 3552.80565591281, 2084.8317031214833, 1457.9927160876075, 1213.0499547157976, 2161.255417921152, 1291.509246342881, 2235.0305071420594, 1887.8113100178664, 1720.188057576579, 1646.7738662390918, 2085.6788518215044, 2343.7550530352796, 1689.7844122316633, 2409.2938550122694, 1885.3868072039083, 1689.7991195486409, 1747.0903029872584, 1605.8962430446395], \"Total\": [13078.0, 8358.0, 4950.0, 22325.0, 4495.0, 5833.0, 7595.0, 10571.0, 3058.0, 6729.0, 5163.0, 2868.0, 5964.0, 5033.0, 5188.0, 2695.0, 3148.0, 2566.0, 5734.0, 11563.0, 2322.0, 5991.0, 1949.0, 6147.0, 2503.0, 13052.0, 1985.0, 2463.0, 2129.0, 3696.0, 2322.3338046823064, 1127.674360157916, 1277.6317169172098, 995.7127279417725, 816.7641467713019, 839.757870174074, 444.87174189810816, 1985.403365023545, 365.8940457661536, 495.85524764637506, 347.89934572272375, 3058.156248442613, 414.8813163991385, 332.9038736636015, 417.8769652486258, 405.8841806069727, 313.9093484575442, 371.89060606619734, 235.93151538816312, 221.93550829112291, 236.9318427233785, 318.91042904975467, 2566.3211327081563, 384.8876297884632, 290.9147603945931, 603.8398390844005, 1313.619969062943, 230.93566153616965, 430.88016717324683, 216.94151141330195, 1392.682775796853, 841.7680754615425, 481.8669475071108, 849.8183285744353, 438.8835179728332, 1068.7609582651442, 3148.9382939633274, 506.81308716385155, 559.792006175353, 1748.096358499936, 922.9606118853333, 442.90024537645706, 8358.548254958932, 1293.7944609931683, 5991.732363368888, 1332.166036986967, 1098.9167432842708, 856.7766422327062, 665.5810582890273, 644.8930718879514, 4385.860623494, 2155.2319304950165, 4257.524085255559, 6443.76090661262, 4852.587173411814, 1801.8812018922067, 2322.506073110193, 2545.244918132088, 7193.796530311472, 2345.8947763123883, 8975.56888605393, 3504.2195646513956, 2931.916018580229, 7365.432607656736, 22325.210114222456, 6864.832495140736, 3874.9885449838694, 8809.979628827863, 7120.120622730779, 6147.240356822176, 10571.155660353805, 6729.8501301563065, 6092.966299306506, 13052.123987519542, 3658.0300926651, 220.17794716530054, 1474.8101325902844, 407.6712245392579, 218.23677093554608, 335.28233517742956, 417.6843782510499, 471.2916673327487, 606.1980728345794, 281.7605839527812, 639.2003037152016, 863.5914872024898, 1032.4419437399188, 4495.186075244983, 341.49127100789804, 746.6601880764811, 527.9922454330773, 358.304496116854, 728.0573106356372, 281.1109100975395, 285.1123182718233, 435.0940024921804, 285.14134273025655, 898.951812176186, 309.85127265371057, 350.5947866809599, 334.7775201984699, 695.5984251654173, 13078.812829485947, 431.2761502604723, 264.3367620108332, 1790.921185675144, 1179.1370848644992, 5163.376632466897, 2199.83481295586, 1668.176630638688, 5833.863509792711, 5033.47704446914, 1680.183970017866, 5188.572468926784, 1243.2762947924234, 5734.765925074937, 2606.21516644494, 5964.144431798957, 1971.2532936288908, 22325.210114222456, 10571.155660353805, 2883.7978790849543, 13052.123987519542, 3732.5067904773764, 1962.6349562870657, 1908.2475645413456, 3735.3144160205466, 7595.0727727786125, 11563.19506226167, 1608.8899822245226, 4416.191907563849, 3850.819154089876, 6092.966299306506, 2660.4194535936026, 8809.979628827863, 5139.667615908259, 7120.120622730779, 8975.56888605393, 4436.052942920783, 7365.432607656736, 4465.06795093489, 3381.4508291316915, 5778.924058627669, 715.3798955546048, 406.48744667183206, 383.3438185247107, 315.9318957915811, 279.7097610640699, 853.2140574156376, 234.43066056902782, 286.7502371049708, 424.5944020653387, 311.89741533942464, 1336.1162083357178, 454.76782204101016, 428.6030595026179, 746.5411209880871, 767.6569431661401, 340.06116959568715, 642.8986921888617, 244.48276066797237, 289.76239091743497, 366.227109678778, 228.38745702229048, 276.6802723737666, 239.4452777023044, 546.2925872882075, 686.1112182408851, 223.338845453744, 236.43182612750905, 223.34835787296825, 829.9437407248646, 396.364504843716, 1075.4762805298028, 1949.6509405239924, 1494.704384872019, 1383.1038460379373, 593.5414268772383, 1565.104713974117, 2868.547978737843, 2695.3122894336607, 818.6721577429139, 1146.2041880211243, 4950.6623399979835, 452.7087234846495, 951.2590452134206, 2129.40257865698, 535.1208464068962, 1300.930499492007, 2503.612675362038, 1143.3949174993377, 1922.325246700615, 973.4380528996872, 2463.9011004192967, 1138.8230710562675, 1141.5752754279783, 1534.4736037080093, 1955.5240536213287, 6729.8501301563065, 2293.7272677928972, 5778.924058627669, 11563.19506226167, 6147.240356822176, 1759.6516661417427, 7595.0727727786125, 3696.5057024540793, 2125.717172724745, 1575.618629269371, 4465.06795093489, 1797.9418725106013, 6864.832495140736, 5139.667615908259, 4416.191907563849, 3978.4960504354613, 8809.979628827863, 13052.123987519542, 4436.052942920783, 22325.210114222456, 8975.56888605393, 7365.432607656736, 13078.812829485947, 5964.144431798957], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.0356, -5.758, -5.6333, -5.8826, -6.0809, -6.0531, -6.6893, -5.1936, -6.8849, -6.581, -6.9354, -4.762, -6.7596, -6.9797, -6.7524, -6.7816, -7.0386, -6.8693, -7.3247, -7.3861, -7.3209, -7.024, -4.9392, -6.8367, -7.1168, -6.3869, -5.6097, -7.3481, -6.7245, -7.4122, -5.5565, -6.0591, -6.6142, -6.0529, -6.7085, -5.8283, -4.774, -6.5685, -6.4771, -5.3894, -6.0036, -6.7047, -3.9602, -5.7053, -4.3172, -5.6923, -5.8696, -6.1017, -6.329, -6.3598, -4.6739, -5.3154, -4.7822, -4.45, -4.6917, -5.511, -5.3116, -5.2397, -4.4323, -5.3052, -4.3757, -5.048, -5.1977, -4.582, -3.9624, -4.7155, -5.0619, -4.5863, -4.7328, -4.8402, -4.6454, -4.9373, -5.0467, -4.8225, -5.2359, -7.3047, -5.4029, -6.6965, -7.3272, -6.9081, -6.6884, -6.5719, -6.3216, -7.0895, -6.2932, -6.0045, -5.8284, -4.3582, -6.9387, -6.1642, -6.5182, -6.9089, -6.219, -7.1771, -7.1681, -6.7499, -7.1763, -6.0294, -7.0994, -6.9763, -7.0234, -6.2991, -3.3657, -6.7784, -7.2704, -5.366, -5.7812, -4.3232, -5.1739, -5.4475, -4.2408, -4.3922, -5.457, -4.3803, -5.7477, -4.3313, -5.0652, -4.3329, -5.332, -3.2107, -3.8769, -5.0317, -3.7267, -4.8235, -5.4037, -5.4364, -4.8936, -4.3892, -4.0844, -5.5814, -4.8478, -4.9742, -4.6819, -5.2903, -4.6583, -4.9831, -4.8373, -4.8299, -5.1965, -5.0812, -5.196, -5.2666, -5.2048, -6.029, -6.5946, -6.6536, -6.8471, -6.9691, -5.8541, -7.1465, -6.9451, -6.5526, -6.8622, -5.4075, -6.4856, -6.5449, -5.9901, -5.9624, -6.7772, -6.1405, -7.108, -6.9383, -6.7045, -7.1771, -6.9853, -7.13, -6.307, -6.0799, -7.2026, -7.1458, -7.2036, -5.8919, -6.6311, -5.6346, -5.0466, -5.3131, -5.392, -6.2307, -5.2726, -4.6927, -4.7701, -5.9263, -5.6078, -4.2048, -6.5039, -5.796, -5.0321, -6.3447, -5.5022, -4.8889, -5.628, -5.1457, -5.7811, -4.9514, -5.6735, -5.6875, -5.4334, -5.2359, -4.2817, -5.1987, -4.5575, -4.0967, -4.5412, -5.4026, -4.4249, -4.9579, -5.3155, -5.4995, -4.9219, -5.4368, -4.8883, -5.0572, -5.1502, -5.1938, -4.9575, -4.8408, -5.168, -4.8133, -5.0585, -5.168, -5.1346, -5.2189], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.0085, 1.0085, 1.0084, 1.0083, 1.0082, 1.0082, 1.0073, 1.0072, 1.0072, 1.0072, 1.0071, 1.0069, 1.0069, 1.0068, 1.0068, 1.0067, 1.0067, 1.0066, 1.0062, 1.006, 1.0058, 1.0055, 1.005, 1.0048, 1.0046, 1.0042, 1.0042, 1.0041, 1.0041, 1.0026, 0.9989, 0.9998, 1.0025, 0.9965, 1.0017, 0.9919, 0.9656, 0.9978, 0.9897, 0.9387, 0.9632, 0.9964, 0.8032, 0.9238, 0.7791, 0.9076, 0.9227, 0.9396, 0.9648, 0.9655, 0.7344, 0.8034, 0.6558, 0.5735, 0.6155, 0.7869, 0.7324, 0.7127, 0.4812, 0.7288, 0.3165, 0.5847, 0.6134, 0.3079, -0.1814, 0.2448, 0.4703, 0.1245, 0.1909, 0.2305, -0.1169, 0.0428, 0.0328, -0.5048, 0.3539, 1.0953, 1.0952, 1.0875, 1.0816, 1.0714, 1.0713, 1.067, 1.0656, 1.0639, 1.041, 1.0289, 1.0264, 1.0255, 1.0224, 1.0146, 1.0072, 1.0041, 0.985, 0.9785, 0.9734, 0.969, 0.9651, 0.9638, 0.9589, 0.9585, 0.9576, 0.9505, 0.95, 0.9493, 0.9468, 0.9379, 0.9407, 0.9219, 0.9244, 0.9275, 0.8822, 0.8783, 0.9108, 0.8599, 0.9212, 0.8088, 0.8636, 0.768, 0.8761, 0.5702, 0.6516, 0.7958, 0.591, 0.7461, 0.8087, 0.8041, 0.6752, 0.47, 0.3544, 0.8298, 0.5536, 0.5642, 0.3977, 0.6179, 0.0524, 0.2665, 0.0865, -0.1378, 0.2004, -0.1913, 0.1944, 0.4018, -0.0723, 1.1926, 1.1922, 1.1918, 1.1918, 1.1916, 1.1913, 1.1907, 1.1907, 1.1906, 1.1896, 1.1894, 1.189, 1.189, 1.1889, 1.1887, 1.1881, 1.188, 1.1873, 1.1871, 1.1867, 1.1863, 1.1862, 1.1861, 1.1843, 1.1835, 1.1831, 1.183, 1.1821, 1.1812, 1.181, 1.1793, 1.1724, 1.1716, 1.1703, 1.1776, 1.1661, 1.1401, 1.1251, 1.1604, 1.1424, 1.0824, 1.1752, 1.1407, 1.0987, 1.1672, 1.1214, 1.0801, 1.1246, 1.0874, 1.1325, 1.0335, 1.0832, 1.0668, 1.0251, 0.9801, 0.6984, 0.8578, 0.575, 0.3422, 0.5294, 0.919, 0.4343, 0.6214, 0.817, 0.9326, 0.4685, 0.8632, 0.0719, 0.1925, 0.2512, 0.312, -0.2467, -0.5231, 0.2289, -1.0323, -0.3663, -0.2781, -0.819, -0.118]}, \"token.table\": {\"Topic\": [1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 2, 3, 1, 3, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 1, 2, 3, 1, 2, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 1, 2, 3, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 1, 2, 3, 1, 2, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 2, 3, 1, 2, 3, 2, 3, 1, 3, 1, 2, 3, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 3, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 3, 1, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 1, 1, 2, 3, 1, 3], \"Freq\": [0.02129467477610534, 0.9618094773874244, 0.017745562313421115, 0.004090267948823115, 0.004090267948823115, 0.993935111564017, 0.1522774812529679, 0.30922711249892454, 0.5383355047477081, 0.11426631098148593, 0.8358871155865988, 0.0499673020902091, 0.0013713039371886602, 0.8032412812082577, 0.19541081104938407, 0.9654318791058824, 0.03182742458590821, 0.800829709796997, 0.07325677179016188, 0.12597944845732384, 0.0013978586848946053, 0.9980711010147483, 0.9959782380402001, 0.003896628474335681, 0.9940262597906004, 0.005424427065705869, 0.0019371549287848554, 0.9278972108879457, 0.06973757743625479, 0.9979013793016707, 0.002393048871227028, 0.0027918593179828252, 0.8492836045303754, 0.14796854385308972, 0.9978215938210491, 0.002463757021780368, 0.4129407959237271, 0.35039808604083167, 0.23677693795956428, 0.9318708274170114, 0.045764067644788776, 0.022309982976834525, 0.004378524166948453, 0.9939249858972988, 0.012087667794584851, 0.00278946179875035, 0.985609835558457, 0.004120554736797879, 0.890039823148342, 0.10576090491114556, 0.009584023270821267, 0.017890176772199697, 0.9724588945459979, 0.01408416596220209, 0.7166157776482349, 0.2692758397059114, 0.995095634043888, 0.0025981609243965744, 0.0025981609243965744, 0.005881294834037897, 0.9939388269524047, 0.9988008321638883, 0.08565349211482473, 0.1957794105481708, 0.7185994273529451, 0.06818693799942595, 0.8377901782196135, 0.09409797443920781, 0.0021989242675789443, 0.0021989242675789443, 0.9939137689456828, 0.002608624299326061, 0.9964944823425552, 0.011547380821037638, 0.9650311114724313, 0.023094761642075277, 0.05869082963003171, 0.06044279469361474, 0.8803624444504756, 0.018386831246067974, 0.8756728380939873, 0.10572427966489086, 0.9556204117945369, 0.0032504095639269964, 0.042255324331050954, 0.9901752387301378, 0.010052540494722212, 0.5834855958288806, 0.12954877006019375, 0.2869685902528594, 0.9956364499540986, 0.0023208308856738896, 0.0023208308856738896, 0.005210660876070915, 0.9952362273295448, 0.997103149485632, 0.06307246869553673, 0.003710145217384514, 0.9334725366939436, 0.07130359046061376, 0.03433883959859586, 0.8942237817822581, 0.06953700098027842, 0.840438580813365, 0.08991853575036002, 0.01464166268508913, 0.925353081697633, 0.06149498327737434, 0.2144335559371112, 0.7620160567504299, 0.02361876848002964, 0.08093674601990541, 0.9070497398782503, 0.013954611382742314, 0.12186589560623273, 0.033236153347154386, 0.8445893085865114, 0.009343683830620364, 0.016818630895116655, 0.973611855150642, 0.7022861034080385, 0.08737472590895998, 0.21021607443150317, 0.0032061823882437203, 0.997122722743797, 0.001172038823444814, 0.001172038823444814, 0.9974050387515369, 0.006902206989898426, 0.9939178065453734, 0.7553620980330081, 0.2387148842542238, 0.005541595527330195, 0.918229313710343, 0.0803837109645418, 0.000772920297735979, 0.8579189917562932, 0.13912199866318267, 0.032154399757127625, 0.5783263167584786, 0.3894758280440811, 0.00026332861577945245, 0.531923803874494, 0.46780328593219733, 0.9976051934314286, 0.002688962785529457, 0.9974149255128507, 0.007114629593373102, 0.8857713843749512, 0.11027675869728308, 0.9982752070277877, 0.0020167175899551268, 0.026843048546644276, 0.9693323086288211, 0.6746916403560614, 0.18711667973222473, 0.13827675341445242, 0.09019796986914036, 0.4968281619021502, 0.4128405440567867, 0.0007673963476807404, 0.7884997472419607, 0.2110339956122036, 0.003507032654138825, 0.8732511308805674, 0.1262531755489977, 0.006241768929281377, 0.7368754985957181, 0.2569528209220833, 0.007983459854135212, 0.08312190554011369, 0.9091751928003395, 0.3040508897909834, 0.5880347791950545, 0.10790491949123143, 0.005410515121543616, 0.4304064779187946, 0.5640462014209219, 0.00723011509847664, 0.015906253216648607, 0.976788549804194, 0.0008898408059297087, 0.9269916595772741, 0.0720771052803064, 0.18869622151497906, 0.0035796031181703344, 0.8074561890844225, 0.012737503973821345, 0.9298377900889583, 0.05673979042884054, 0.006107450891923627, 0.02687278392446396, 0.9674202212807026, 0.5898137349481428, 0.20239660572342585, 0.2078179433767319, 0.993429878647182, 0.002278508895979775, 0.00455701779195955, 0.44128465885384804, 0.36249385884843466, 0.1962045411899509, 0.09953996225113178, 0.4738309758244579, 0.4266121926888211, 0.4590677826462664, 0.02651596334916422, 0.5143771540494311, 0.0068048989736554, 0.8596345973124456, 0.13357481468512342, 0.01787105214456694, 0.03469086592768877, 0.948217002023493, 0.010202427556784806, 0.9896354730081262, 0.03670942040832364, 0.8721268363674465, 0.09121734768128904, 0.003783053073635734, 0.8587530477153116, 0.13997296372452217, 0.003551040357230634, 0.7984767888972882, 0.19835096852531112, 0.9990644217497516, 0.11553248013439368, 0.17002893302797562, 0.7145574903406462, 0.9970503269723806, 0.7433469315748626, 0.11433084412700054, 0.1422259985359938, 0.9805863561975202, 0.018328716938271403, 0.980721399990888, 0.014291022222089443, 0.005359133333283541, 0.903791244162892, 0.014262486411208428, 0.08182163256956414, 0.46570692034554273, 0.20874507877859327, 0.3255724013050635, 0.032780521008098064, 0.8002420522461757, 0.16688265240486286, 0.9885301163070159, 0.009865570023024111, 0.9982858067617385, 0.001511027961798797, 0.03584470066235602, 0.8662469326736038, 0.09857292682147906, 0.9570584860655454, 0.042068504882002, 0.9957847741520669, 0.08635375323968422, 0.014565693317537097, 0.8989113590251465, 0.08933164249862127, 0.5845509513499898, 0.326164369122873, 0.013253555075802396, 0.004417851691934132, 0.9807630756093774, 0.9978757385201443, 0.002410327870821605, 0.9972848809067484, 0.9976599467583589, 0.001961966463634924, 0.9833817299108919, 0.002806988762828426, 0.01403494381414213, 0.08333455724128996, 0.9109982280241017, 0.005681901630087952, 0.7597140643620269, 0.16758398478574124, 0.07273372945122647, 0.09450342345640396, 0.6532247966958235, 0.252455321018099, 0.0026790218834200226, 0.0026790218834200226, 0.9952566296905384, 0.8803548072612529, 0.11925124879236094, 0.49596543673517884, 0.2745256263560174, 0.22945020204830338, 0.813897317152406, 0.008613936033363698, 0.17754279268766288, 0.38054339256742387, 0.010252828616615481, 0.6092260482336735, 0.9172669414312926, 0.021839689081697443, 0.06096913201973869, 0.9418018053198918, 0.057884828566172754, 0.8596914230474173, 0.13944827430702253, 0.07745651581306355, 0.8681584480714207, 0.05486503203425335, 0.008955002860952027, 0.9895278161351989, 0.9952970325894586, 0.004968204821577996, 0.06845509662937437, 0.8670978906387419, 0.06560280093648377, 0.9981629511771947, 0.9946500220368747, 0.0045417809225428065, 0.07293469190639755, 0.8514701241165482, 0.07547892534499281, 0.6732798577757023, 0.2919592493698081, 0.03513061061342317, 0.00845909805273588, 0.989714472170098, 0.04972935938962866, 0.9492200529108068, 0.9872698337860246, 0.011767220903289924, 0.36973770524140487, 0.21641343590268497, 0.4139755272145437, 0.22004081502337935, 0.6003620565888581, 0.17958762897451294, 0.9980404646642901, 0.0007484378931721903, 0.004490627359033142, 0.9954223979190132, 0.008424011827289262, 0.008424011827289262, 0.9839245814273857, 0.03860766926953598, 0.8348908479537156, 0.12627925156910727, 0.9866781618704186, 0.0022578447640055344, 0.011289223820027672, 0.007228560181906304, 0.003614280090953152, 0.9939270250121167, 0.012049009480168016, 0.9868138764257606, 0.19865097829280026, 0.43407476255752925, 0.3673389294973623, 0.996051755160261, 0.06215661833272322, 0.7011373714514512, 0.23683814916434193, 0.9975565446431861, 0.3766638263305698, 0.4948328698852583, 0.1285088348657238, 0.01846484375830094, 0.0030774739597168236, 0.9786367191899499, 0.9959483886986286, 0.004330210385646211, 0.008352639146580056, 0.9939640584430265, 0.9994019903424766, 0.9574655704685945, 0.00031756735338925193, 0.042236458000770506, 0.05711746126198267, 0.05072669636553706, 0.8923105486662188, 0.210268888507503, 0.03011959754296665, 0.7592411758000649, 0.048456571418812315, 0.004183301129681638, 0.9475177058728912, 0.004666322266390131, 0.9939266427410979, 0.9174186744369914, 0.08303643476655981, 0.3243732388560094, 0.6378678184911261, 0.03783881468136592, 0.9968555724248838, 0.0034374330083616683, 0.0034374330083616683, 0.008954621466872486, 0.004477310733436243, 0.9894856720894097, 0.025366458863779414, 0.044583473154521394, 0.9301034916719119, 0.13689497536824058, 0.17735190966950756, 0.6858861652470611, 0.0036610418053226487, 0.005491562707983973, 0.9903118083397765, 0.9325651057874969, 0.028011968133792146, 0.039683621522872206, 0.06146696688808248, 0.043904976348630345, 0.8947834179850864, 0.000669028612025931, 0.021408915584829793, 0.9781198307819112, 0.014776053950889836, 0.7464454840708141, 0.2384549396212567, 0.18656383924425043, 0.7855725297102941, 0.027946029638859827, 0.06653086217698286, 0.6168200273018579, 0.31649144606225743, 0.015866783709959517, 0.21451891575865267, 0.7698563456072358, 0.7582326782214825, 0.19117280473045897, 0.05037661746275608, 0.0524753075964541, 0.013993415359054425, 0.933185886756942, 0.003652744015767685, 0.14489217929211817, 0.8514952161200671, 0.039036895965598645, 0.02054573471873613, 0.9399673633821779, 0.0015554550229295444, 0.004666365068788633, 0.9939357596519789, 0.9990975134607349, 0.9973836565488309, 0.9567477569478406, 0.01085451263960273, 0.03256353791880819, 0.5002468430693543, 0.2896752320668867, 0.21001454324849286, 0.9974625087756329, 0.9994256619441971, 0.00043060131923489754, 0.005461092166973174, 0.9939187743891177, 0.8142975125637217, 0.08722959108944711, 0.098829270755597, 0.07546190686983056, 0.7430901662598592, 0.18184223391549448, 0.11220433944238212, 0.4038012455381536, 0.48397919667662226, 0.03836250735850657, 0.7464995181898483, 0.21517879127453232, 0.9940503337654872, 0.0020752616571304535, 0.004150523314260907, 0.9956600679733062, 0.004609537351728269, 0.7945949036553926, 0.15488008203995052, 0.05056968195916488, 0.002522930251774925, 0.0100917210070997, 0.9864657284439957, 0.0023551888464278786, 0.9962448820389926, 0.0023941522644137492, 0.9696316670875683, 0.026335674908551238, 0.519405240489898, 0.36249018362610774, 0.1180963494166505, 0.9995055563282867, 0.9949605142896348, 0.002283765526296025, 0.003045020701728033, 0.9907717152883433, 0.003563927033411307, 0.005939878389018844, 0.0024529570394137595, 0.9860887298443314, 0.012264785197068797, 0.2128017884697409, 0.406216973328891, 0.38096930351044717, 0.6540686043535663, 0.16865381552048764, 0.17721492290731444, 0.0005951729202543021, 0.8266951862332256, 0.1726001468737476, 0.9960670431096658, 0.9992842032427909, 0.6468272272055869, 0.1928997704934128, 0.16031010693459005, 0.9971451888467008, 0.0031356766944864805], \"Term\": [\"accommodating\", \"accommodating\", \"accommodating\", \"al\", \"al\", \"al\", \"also\", \"also\", \"also\", \"always\", \"always\", \"always\", \"amazing\", \"amazing\", \"amazing\", \"ambiance\", \"ambiance\", \"another\", \"another\", \"another\", \"asada\", \"asada\", \"asked\", \"asked\", \"atmosphere\", \"atmosphere\", \"attentive\", \"attentive\", \"attentive\", \"attitude\", \"attitude\", \"awesome\", \"awesome\", \"awesome\", \"awful\", \"awful\", \"back\", \"back\", \"back\", \"bad\", \"bad\", \"bad\", \"bean\", \"bean\", \"beans\", \"beans\", \"beans\", \"beautiful\", \"beautiful\", \"beautiful\", \"beef\", \"beef\", \"beef\", \"best\", \"best\", \"best\", \"bill\", \"bill\", \"bill\", \"birria\", \"birria\", \"boba\", \"bread\", \"bread\", \"bread\", \"breakfast\", \"breakfast\", \"breakfast\", \"brisket\", \"brisket\", \"brisket\", \"broth\", \"broth\", \"brunch\", \"brunch\", \"brunch\", \"burrito\", \"burrito\", \"burrito\", \"cafe\", \"cafe\", \"cafe\", \"call\", \"call\", \"call\", \"called\", \"called\", \"came\", \"came\", \"came\", \"card\", \"card\", \"card\", \"carne\", \"carne\", \"charged\", \"cheese\", \"cheese\", \"cheese\", \"chicken\", \"chicken\", \"chicken\", \"clean\", \"clean\", \"clean\", \"cocktails\", \"cocktails\", \"cocktails\", \"coffee\", \"coffee\", \"coffee\", \"comfortable\", \"comfortable\", \"comfortable\", \"cooked\", \"cooked\", \"cooked\", \"corn\", \"corn\", \"corn\", \"could\", \"could\", \"could\", \"creamy\", \"creamy\", \"crispy\", \"crispy\", \"crispy\", \"crunchy\", \"crunchy\", \"customer\", \"customer\", \"customer\", \"customers\", \"customers\", \"customers\", \"cute\", \"cute\", \"definitely\", \"definitely\", \"definitely\", \"delicious\", \"delicious\", \"delicious\", \"dirty\", \"dirty\", \"disgusting\", \"efficient\", \"efficient\", \"efficient\", \"employee\", \"employee\", \"environment\", \"environment\", \"even\", \"even\", \"even\", \"everything\", \"everything\", \"everything\", \"excellent\", \"excellent\", \"excellent\", \"exceptional\", \"exceptional\", \"exceptional\", \"favorite\", \"favorite\", \"favorite\", \"flavor\", \"flavor\", \"flavor\", \"food\", \"food\", \"food\", \"fresh\", \"fresh\", \"fresh\", \"fried\", \"fried\", \"fried\", \"friendly\", \"friendly\", \"friendly\", \"fries\", \"fries\", \"fries\", \"fun\", \"fun\", \"fun\", \"garlic\", \"garlic\", \"garlic\", \"get\", \"get\", \"get\", \"girl\", \"girl\", \"girl\", \"go\", \"go\", \"go\", \"good\", \"good\", \"good\", \"got\", \"got\", \"got\", \"great\", \"great\", \"great\", \"green\", \"green\", \"green\", \"grilled\", \"grilled\", \"helpful\", \"helpful\", \"helpful\", \"hidden\", \"hidden\", \"hidden\", \"highly\", \"highly\", \"highly\", \"horrible\", \"hot\", \"hot\", \"hot\", \"juicy\", \"know\", \"know\", \"know\", \"knowledgeable\", \"knowledgeable\", \"lady\", \"lady\", \"lady\", \"left\", \"left\", \"left\", \"like\", \"like\", \"like\", \"love\", \"love\", \"love\", \"management\", \"management\", \"manager\", \"manager\", \"margaritas\", \"margaritas\", \"margaritas\", \"mask\", \"mask\", \"mcdonald\", \"meat\", \"meat\", \"meat\", \"menu\", \"menu\", \"menu\", \"milk\", \"milk\", \"milk\", \"min\", \"min\", \"mins\", \"minutes\", \"minutes\", \"money\", \"money\", \"money\", \"music\", \"music\", \"music\", \"never\", \"never\", \"never\", \"nice\", \"nice\", \"nice\", \"noodles\", \"noodles\", \"noodles\", \"notch\", \"notch\", \"one\", \"one\", \"one\", \"order\", \"order\", \"order\", \"ordered\", \"ordered\", \"ordered\", \"orders\", \"orders\", \"orders\", \"outdoor\", \"outdoor\", \"outstanding\", \"outstanding\", \"owned\", \"owned\", \"owned\", \"pad\", \"pad\", \"paid\", \"paid\", \"pancakes\", \"pancakes\", \"pancakes\", \"pastor\", \"pastries\", \"pastries\", \"patio\", \"patio\", \"patio\", \"people\", \"people\", \"people\", \"peppers\", \"peppers\", \"perfectly\", \"perfectly\", \"phone\", \"phone\", \"pizza\", \"pizza\", \"pizza\", \"place\", \"place\", \"place\", \"poor\", \"pork\", \"pork\", \"pork\", \"potato\", \"potato\", \"potato\", \"prices\", \"prices\", \"prices\", \"problem\", \"problem\", \"problem\", \"quesadilla\", \"quesadilla\", \"quesadilla\", \"ramen\", \"ramen\", \"really\", \"really\", \"really\", \"receipt\", \"recommend\", \"recommend\", \"recommend\", \"refund\", \"restaurant\", \"restaurant\", \"restaurant\", \"rice\", \"rice\", \"rice\", \"ridiculous\", \"ridiculous\", \"roasted\", \"roasted\", \"rude\", \"said\", \"said\", \"said\", \"salad\", \"salad\", \"salad\", \"sandwich\", \"sandwich\", \"sandwich\", \"sauce\", \"sauce\", \"sauce\", \"seasoned\", \"seasoned\", \"selection\", \"selection\", \"service\", \"service\", \"service\", \"showed\", \"showed\", \"showed\", \"shredded\", \"shredded\", \"shredded\", \"shrimp\", \"shrimp\", \"shrimp\", \"side\", \"side\", \"side\", \"soft\", \"soft\", \"soft\", \"someone\", \"someone\", \"someone\", \"soup\", \"soup\", \"soup\", \"spicy\", \"spicy\", \"spicy\", \"spot\", \"spot\", \"spot\", \"staff\", \"staff\", \"staff\", \"super\", \"super\", \"super\", \"sweet\", \"sweet\", \"sweet\", \"table\", \"table\", \"table\", \"taco\", \"taco\", \"taco\", \"tacos\", \"tacos\", \"tacos\", \"tea\", \"tea\", \"tea\", \"tender\", \"tender\", \"tender\", \"terrible\", \"texture\", \"thru\", \"thru\", \"thru\", \"time\", \"time\", \"time\", \"tofu\", \"told\", \"told\", \"tomato\", \"tomato\", \"took\", \"took\", \"took\", \"town\", \"town\", \"town\", \"try\", \"try\", \"try\", \"tucson\", \"tucson\", \"tucson\", \"understand\", \"understand\", \"understand\", \"upset\", \"upset\", \"us\", \"us\", \"us\", \"veggie\", \"veggie\", \"veggie\", \"veggies\", \"veggies\", \"vibe\", \"vibe\", \"vibe\", \"wait\", \"wait\", \"wait\", \"waited\", \"waiting\", \"waiting\", \"waiting\", \"walked\", \"walked\", \"walked\", \"welcoming\", \"welcoming\", \"welcoming\", \"well\", \"well\", \"well\", \"went\", \"went\", \"went\", \"wonderful\", \"wonderful\", \"wonderful\", \"worse\", \"worst\", \"would\", \"would\", \"would\", \"zero\", \"zero\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 3, 2]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el1691373201387769608181189967\", ldavis_el1691373201387769608181189967_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el1691373201387769608181189967\", ldavis_el1691373201387769608181189967_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el1691373201387769608181189967\", ldavis_el1691373201387769608181189967_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Mn9p3HUJ2FK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ca2abb6-cec0-4779-d617-193b84666f0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: busier, banana, compare, cod, basil, contained, burgers, admit, cheers, brussel\n",
            "Topic 2: bean, ample, business, areas, beans, accommodate, changes, amount, banana, canelo\n",
            "Topic 3: banana, beats, canelo, cheers, bean, accompanied, accommodating, commented, african, baristas\n",
            "Review 1: pandemic pit stop ice cream plain sundae limited menu written screens outside unpleasant surprise cashier wearing gloves mask holding item good since hold lid three customers pm location bomb parking access easy great visibility pictures tonight soo\n",
            "Topic Distribution: [0.57368238 0.23334336 0.19297426]\n",
            "==================================================\n",
            "Review 2: lucky enough go soft opening let tell good beer wine many different modern italian appetizers bruschettas paninis salads please favor visit place good staff friendly look forward enjoying restaurant future\n",
            "Topic Distribution: [0.01602784 0.14795039 0.83602177]\n",
            "==================================================\n",
            "Review 3: gone claim jumpers us never disappoint location different cook food well excellent desserts service deliver everything impeccable would absolutely recommend restaurant quality meal best place tucson take wife dinner hands\n",
            "Topic Distribution: [0.1227805  0.01409964 0.86311986]\n",
            "==================================================\n",
            "Review 4: stay main hotel casino july july worst experience ever years supported hotel casino however time disaster go back hopefullly\n",
            "Topic Distribution: [0.94176394 0.02807469 0.03016137]\n",
            "==================================================\n",
            "Review 5: town long weekend hiking camping burgers well seasoned cooked ordered lots flavor waitresses super friendly attentive expecting bar scene pleasantly surprised relaxing dinner locals\n",
            "Topic Distribution: [0.02137743 0.49485688 0.48376569]\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# Code Cell 10: Show the top 10 words for each topic and document topic distribution of the first 5 reviews based on the best model\n",
        "for i, topic in enumerate(best_model.components_):\n",
        "    top_words_idx = topic.argsort()[::-1][:10]\n",
        "    top_words = [feature_names[idx] for idx in top_words_idx]\n",
        "    print(f\"Topic {i+1}: {', '.join(top_words)}\")\n",
        "\n",
        "document_topic_distribution_best = best_model.transform(X[:5])\n",
        "\n",
        "for i, review in enumerate(selected_reviews['processed_text'][:5]):\n",
        "    print(f\"Review {i+1}: {review}\")\n",
        "    print(\"Topic Distribution:\", document_topic_distribution_best[i])\n",
        "    print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bKlQTZkK3kL"
      },
      "source": [
        "\n",
        "The first topic appears to cover aspects like crowd levels, food ingredients, and the overall restaurant atmosphere. It might capture reviews discussing the busy environment, specific dishes, and the overall vibe. The second topic revolves around business-related elements, including the overall environment, changes, and the variety of items available such as beans and banana. It likely touches on aspects related to the restaurant's offerings and flexibility. The third topic is a bit more complex, suggesting an emphasis on entertainment or unique experiences within the restaurant. Words like \"beats,\" \"cheers,\" and \"accompanying\" hint at a lively atmosphere, possibly with cultural influences. The reviews vary in sentiment, with one expressing negative sentiments linked to the pandemic, another providing a positive outlook on a soft opening, and others praising specific restaurants for their quality service and diverse offerings. Yet, a negative experience during July at a hotel casino stands out, contrasting with a positive surprise during a town visit, highlighting well-seasoned burgers and friendly service.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiIDnOALK6M9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1509729-6667-4da3-e1b0-d454912cd8b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.4/123.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q jupyter\n",
        "!pip install -q nbconvert"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}